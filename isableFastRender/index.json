[{"authors":["admin"],"categories":null,"content":"I am a masters student at TUM, focusing on Machine Learning, Robotics and Software Engineering. My research interests lie in designing intelligent deep models and applying them in various paradigms. I have a keen interest in using deep learning to improve reinforcement learning systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:1313/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a masters student at TUM, focusing on Machine Learning, Robotics and Software Engineering. My research interests lie in designing intelligent deep models and applying them in various paradigms. I have a keen interest in using deep learning to improve reinforcement learning systems.","tags":null,"title":"Rafid Abyaad","type":"authors"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"b6afff65cefee68f1ef97e7a75460306","permalink":"http://localhost:1313/project/a3c/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/a3c/","section":"project","summary":"Implementation of Advantage Actor Critic to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning"],"title":"Advantage Actor Critic","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"44d73dd2341776ad24b5af09172e1979","permalink":"http://localhost:1313/project/dqn/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/dqn/","section":"project","summary":"Implementation of Deep Q Learning to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning"],"title":"Deep Q Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"11b36ddbf4444780eb53371c6c17dae6","permalink":"http://localhost:1313/project/ddqn/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/ddqn/","section":"project","summary":"Implementation of Double Deep Q Learning to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning"],"title":"Double Deep Q Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"e3c9df9bcde44c5e1ac3b0bfa9f68e6f","permalink":"http://localhost:1313/project/duellingdqn/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/duellingdqn/","section":"project","summary":"Implementation of Dueling Deep Q Learning to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning"],"title":"Dueling Deep Q Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"c31dfbe4ed44b15c1297c1bb5a4629c4","permalink":"http://localhost:1313/project/d3qn/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/d3qn/","section":"project","summary":"Implementation of Dueling Double Deep Q Learning to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning"],"title":"Dueling Double Deep Q Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"e0422b3d74b12c850079abd22d7e0a25","permalink":"http://localhost:1313/project/reinforce/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/reinforce/","section":"project","summary":"Implementation of REINFORCE to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning","Policy Gradient"],"title":"Policy Gradient (REINFORCE)","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"0999d97abbf45d09dd9473fc45f4d526","permalink":"http://localhost:1313/project/sac/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/sac/","section":"project","summary":"Implementation of SAC to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning","Policy Gradient"],"title":"Soft Actor Critic","type":"project"},{"authors":null,"categories":null,"content":"","date":1590343200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590343200,"objectID":"908bcec67591270e2510435168c5d2d8","permalink":"http://localhost:1313/project/valueac/","publishdate":"2020-05-25T00:00:00+06:00","relpermalink":"/project/valueac/","section":"project","summary":"Implementation of Value Actor Critic to solve OpenAI LunarLander environment","tags":["Deep Learning","Reinforcement Learning","Policy Gradient"],"title":"Value Actor Critic","type":"project"},{"authors":null,"categories":null,"content":"This was one my of very first competitions. While doing Practical Deep Learning for Coders, this competition provided a good source of practise. It was a binary classification problem. The goal for this competition was to determine whether the given satellite image contained a columnur cactus.\nI used this dataset for two purposes :\n To implement and test ArcFace using pytorch. To get placed into a high LeaderBoard position in the competition using FastAI.  Approach EDA According to the dataset details, the images were taken from the air. The images are low-res, some of them rotated to arbitrary angles and some zoomed. From visual inspection, the cacti are somewhat easy to spot because of their unique texture and stick-like shape. The class imbalance is not severe, can be handled by data augmentation.\nData split and Transforms Split As the class imbalance was not servere, the data could be split into train/valid set at random.\nTransforms Following Transforms were applied with 75% probability to augment the data, then the images were resized to 128*128. Test time augmentation was not applied.\n Horizontal Flip Vertical Flip Left and Right rotation upto 10Â° Upto 110% zoom  Hyperparameters ArcFace  s = 64 m = 0.0 Adam Optimizer with fixed lr = 1e-3  Competition Classifiers Densenet169  Frozen model, Adam optimizer with maximum lr = 7.5e-3. CyclirLR scheduler Unfrozen model, Adam optimizer with maximum lr = 1e-6.  Resnet101  Frozen model, Adam optimizer with maximum lr = 9e-3. CyclirLR scheduler Unfrozen model, Adam optimizer with maximum lr = 1e-6.  Model Performance I used DenseNet169 and Resnet101 for Leaderboard and ArcFace for research purposes.\nArcFace ArcFace was applied on the Resnet101 backbone. Implemented from scratch in pytorch. With embedding dimension = 2048 and scale_factor (s) = 64, accuracy follows :\nFurther experimentation using additional linear layers can boost the results. Then again, this approach is designed for one-shot learning. Worse performance in Binary Classification is quite understandable.\nDenseNet169 Densenet169 needs more time to converge because of its enormous size and paramters.\n   epoch train_loss valid_loss error_rate accuracy time     0 0.059754 0.004154 0.000000 1.000000 01:35   1 0.062731 0.000837 0.000000 1.000000 01:29   2 0.019187 0.003954 0.000000 1.000000 01:29   3 0.009922 0.000457 0.000000 1.000000 01:26   4 0.004491 0.000055 0.000000 1.000000 01:27    Resnet101 Resnet101 needed less time to converge.\n   epoch train_loss valid_loss error_rate accuracy time     0 0.063169 0.033260 0.011429 0.988571 01:17   1 0.034835 0.002770 0.000000 1.000000 01:15   2 0.024171 0.002123 0.000000 1.000000 01:15   3 0.014281 0.006416 0.005714 0.994286 01:14   4 0.006923 0.002465 0.000000 1.000000 01:13    Competition Standings My models acheived perfect accuracy score in the public leaderboard.\n","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"8d5e5566ce7ce6a7ce03de14fc5b29c8","permalink":"http://localhost:1313/project/acic/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/acic/","section":"project","summary":"Approaches and code for identifying cacti from satellite images","tags":["Deep Learning","Computer Vision"],"title":"Aerial Cactus Identification","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"2e8099fdeae20f08476661bf7e6574e4","permalink":"http://localhost:1313/project/aptos/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/aptos/","section":"project","summary":"Description of approaches for the APTOS medical image classification Competition","tags":["Deep Learning","Computer Vision"],"title":"APTOS Blindness Detection","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"42a36e94cfdd683e6c065cf82a611878","permalink":"http://localhost:1313/project/f360/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/f360/","section":"project","summary":"Classifying fruits using computer vision models","tags":["Deep Learning","Computer Vision"],"title":"Computer Vision 101","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"cc85f6ff0839f6d222f0b11bba9c684d","permalink":"http://localhost:1313/project/ferm/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/ferm/","section":"project","summary":"Vision model to classify facial expression","tags":["Deep Learning","Computer Vision"],"title":"Facial Expression Recognition","type":"project"},{"authors":null,"categories":null,"content":"The problem for this competition was text binary classification. The goal of this competition was to develop a model that can flag potentially toxic/insincere questions. Being a kernel-only competition, models available memory and execution time was limited for submissions. I developed a model and submitted it to the competition after it had ended. The submissions placed me at :\n Public LB : Top 50 Private LB : Top 100  Aproach EDA Initial EDA of the data revealed the dataset to be very imbalanced. Around 96% of the data was from one class and the rest from the other. So accuracy as a metric was useless in this particular competition. The evaluation criteria was to maximized F-score, rightly so.\nText Preprocessing and Loading The competition provided word embedding files. To use the word embeddings, I needed to make sure that word coverage is as much as possible. That meant I needed to fix word misspellings and typos. Glove and Paragram word embeddings do not include many punctuations present in the dataset, so I needed to remove them as well.\nProcessing Steps  Cleaning Text from numbers and Punctuations Replacing typical misspellings and typos Filling blank questions with special tokens  Crafting Meta Features Based on some EDA, these features are extracted from the train data :\n Total length Capitals Ratio of capitals and length Number of words Number of unique words Ratio of unique words and total words  Tokenization I used keras tokenizer. All out of vocabulary words will be replaced with special \u0026lsquo;xxunk\u0026rsquo; token. Tokenizer is fit on the train data and used to make sequence of texts. Then the sequences are padded so all of them have the same length. Longer sequences are truncated and shorter ones are padded.\nScaling Statistical Features Calculated Statistical Features will have to be scaled before putting through a neuralnet.\nAs because these features have different ranges for values, some features may produce vanishing gradients. For example number of words will be, on average, a way larger value than number of unique words. So, these feaures are scaled using a standard scaler.\nLoading Embeddings I have used paragram and glove embeddings in the model. As these word embeddings were not trained in exactly the same system, some useful information may be lost if a weighted average is used. That is why, I have concatenated them into a num_words * (embedding_dim * num_embedding) matrix.\nUsing more embeddings may end up giving more generalization but these models are quite memory intensive and used all my ram. So I couldn't use more than two of them.\nModel Definition Modules Following Modules contruct the model :\n Embedding and Dropout LSTM+GRU backbone Attention Block Extractor/ Merge Layer Output/ Head  Embedding and Dropout This layer takes truncated sequence of tokenized texts as input and provides a 2d array as output. In my case I limited 120000 as the maximum number of unique features to use. A dropout of 10% was applied. This module returns an array of size (120000 * 300 * 2)\nLSTM+GRU and Attention Backbone The main body of the model consists of bidirectional LSTM and GRU (128). The body returns LSTM+GRU outputs and Attention-transformed LSTM+GRU outputs.\nExtractor/ Merge This layer serves as the feature extractor for lstm, gru outputs and statistical Meta features. Conv1d is applied on lstm output, which goes into a maxpool. GRU output is sent through avgpool. Statistical Features are put through a Linear layer.\nOutput Layer This layer consists of a dropout, a relu and a batchnorm layer, sandwiched between two linear layers. The last linear layer provides the output of the model as a score for the input.\nComplete Model  First input is sent through embedding layer, returning feature vectors. Embedding output is then passed to the Body of the model. Extractor layer is used to find features from the sequential output of the model backbone. Attention infused lstm+gru outputs and extracted features are concatenated. Concatenated features are sent to the output layer.  Training the model The model was trained 5 epochs with Stratified 5-fold-cross-validation. Adam optimizer with 0.01 learning rate and ExponentialLR scheduler was used.\nBest threshold for classification The sigmoid value provided by the model is a continuous representation of insincerity in the inputs. We need a specific threshold to separate the two classes. The best threshold was calculated on the training data, then it was applied on the test data. Based on the threshold of the score, test data was classified and submitted.\n","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"18caf11feb3d42b782388c06871ef47b","permalink":"http://localhost:1313/project/qiqc/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/qiqc/","section":"project","summary":"Approaches and codes for short-text binary classification","tags":["Deep Learning","Natural Language Processing"],"title":"Quora Insincere Questions Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"3120f353182d7ece615b5b2ccb8efe44","permalink":"http://localhost:1313/project/rcic/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/rcic/","section":"project","summary":"Recursion Cellular Image Classification","tags":["Deep Learning","Computer Vision"],"title":"Recursion Cellular Image Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"9ef400c9f2227de4b0609d72306fcc5c","permalink":"http://localhost:1313/project/siim-acr/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/siim-acr/","section":"project","summary":"SIIM-ACR Pneumothorax Segmentation","tags":["Deep Learning","Computer Vision"],"title":"SIIM-ACR Pneumothorax Segmentation","type":"project"},{"authors":null,"categories":null,"content":"","date":1577210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577210400,"objectID":"0e59012bbcdad557cc798c391f506609","permalink":"http://localhost:1313/project/dmnist/","publishdate":"2019-12-25T00:00:00+06:00","relpermalink":"/project/dmnist/","section":"project","summary":"Performance analysis of Tabular vs Vision models on Digits-MNIST dataset","tags":["Deep Learning","Computer Vision","Tabular Model"],"title":"Tabular vs Vision Models","type":"project"}]